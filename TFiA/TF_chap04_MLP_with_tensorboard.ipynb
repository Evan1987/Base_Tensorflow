{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "log_dir = \"F:/tmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 784], name=\"x_input\")\n",
    "    y_ = tf.placeholder(dtype=tf.float32, shape=[None, 10], name=\"y_input\")\n",
    "\n",
    "with tf.name_scope(\"input_reshape\"):\n",
    "    image_shaped_input = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.summary.image(name=\"input\", tensor=image_shaped_input, max_outputs=10)\n",
    "\n",
    "# 参数初始化函数\n",
    "# 其中 W 的 shape 为 长度为 4 的列表： \n",
    "# [height * width * in_channels, num of kernels(output_channels)]\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "# b 的 shape 为长度为 1 的列表： [output_channels]\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(value=0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        # 计算参数的均值，并使用tf.summary.scaler记录\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(name=\"mean\", tensor=mean)\n",
    "        # 计算参数的标准差\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(name=\"stddev\", tensor=stddev)\n",
    "        tf.summary.scalar(name=\"max\", tensor=tf.reduce_max(var))\n",
    "        tf.summary.scalar(name=\"min\", tensor=tf.reduce_min(var))\n",
    "        # 用直方图记录参数的分布\n",
    "        tf.summary.histogram(name=\"histogram\", values=var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"\n",
    "    Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # 设置命名空间\n",
    "    with tf.name_scope(layer_name):\n",
    "        # 调用之前的方法初始化权重w，并且调用参数信息的记录方法，记录w的信息\n",
    "        with tf.name_scope(\"weights\"):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        # 调用之前的方法初始化权重b，并且调用参数信息的记录方法，记录b的信息\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        # 执行wx+b的线性计算，并且用直方图记录下来\n",
    "        with tf.name_scope(\"linear_compute\"):\n",
    "            Z = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram(name=\"linear\", values=Z)\n",
    "        # 将线性输出经过激励函数，并将输出也用直方图记录下来\n",
    "        activations = act(Z, name=\"activation\")\n",
    "        tf.summary.histogram(name=\"activations\", values=activations)\n",
    "        \n",
    "        # 返回激励层的最终输出\n",
    "        return activations\n",
    "\n",
    "hidden1 = nn_layer(input_tensor=X, \n",
    "                   input_dim=784, \n",
    "                   output_dim=500, \n",
    "                   layer_name=\"layer1\", \n",
    "                   act=tf.nn.relu)\n",
    "\n",
    "# 创建一个 dropout 层， 随机关闭掉 hidden1的一些神经元\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    prob = tf.placeholder(dtype=tf.float32)\n",
    "    tf.summary.scalar(name=\"dropout_keep_prob\", tensor=prob)\n",
    "    dropped = tf.nn.dropout(x=hidden1, keep_prob=prob)\n",
    "\n",
    "# 创建一个输出层\n",
    "# 输入的维度是上一层的输出:500,\n",
    "# 输出的维度是分类的类别种类：10，\n",
    "# 激活函数设置为全等映射identity.（暂且先别使用softmax,会放在之后的损失函数中一起计算）\n",
    "y = nn_layer(input_tensor=dropped, \n",
    "             input_dim=500, \n",
    "             output_dim=10, \n",
    "             layer_name=\"layer2\", \n",
    "             act=tf.identity)       \n",
    "\n",
    "# 计算损失， 并用 tf.summary记录\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # 计算交叉熵损失（每个样本都会有一个损失）\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    with tf.name_scope(\"total\"):\n",
    "        # 计算所有样本交叉熵损失的均值\n",
    "        cross_entropy = tf.reduce_mean(diff)\n",
    "tf.summary.scalar(name=\"loss\", tensor=cross_entropy)\n",
    "\n",
    "# 设置训练\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    \n",
    "# 计算准确率,并用 tf.summary记录\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    with tf.name_scope(\"correct_prediction\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(name=\"accuracy\", tensor=accuracy)\n",
    "\n",
    "\n",
    "# 将所有的summaries合并，并且将它们写到之前定义的log_dir路径\n",
    "merged = tf.summary.merge_all()\n",
    "#merged = tf.summary.merge(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "\n",
    "def generate_feed(train=True):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        k = 0.9\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {X: xs, y_: ys, prob: k} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc at step 0: 0.1550\n",
      "test acc at step 10: 0.7251\n",
      "test acc at step 20: 0.8296\n",
      "test acc at step 30: 0.8649\n",
      "test acc at step 40: 0.8811\n",
      "test acc at step 50: 0.8923\n",
      "test acc at step 60: 0.9008\n",
      "test acc at step 70: 0.9049\n",
      "test acc at step 80: 0.9098\n",
      "test acc at step 90: 0.9140\n",
      "Adding run metadata for  99\n",
      "test acc at step 100: 0.9161\n",
      "test acc at step 110: 0.9194\n",
      "test acc at step 120: 0.9225\n",
      "test acc at step 130: 0.9263\n",
      "test acc at step 140: 0.9239\n",
      "test acc at step 150: 0.9275\n",
      "test acc at step 160: 0.9301\n",
      "test acc at step 170: 0.9296\n",
      "test acc at step 180: 0.9335\n",
      "test acc at step 190: 0.9325\n",
      "Adding run metadata for  199\n",
      "test acc at step 200: 0.9368\n",
      "test acc at step 210: 0.9376\n",
      "test acc at step 220: 0.9359\n",
      "test acc at step 230: 0.9381\n",
      "test acc at step 240: 0.9390\n",
      "test acc at step 250: 0.9358\n",
      "test acc at step 260: 0.9375\n",
      "test acc at step 270: 0.9433\n",
      "test acc at step 280: 0.9424\n",
      "test acc at step 290: 0.9432\n",
      "Adding run metadata for  299\n",
      "test acc at step 300: 0.9409\n",
      "test acc at step 310: 0.9473\n",
      "test acc at step 320: 0.9458\n",
      "test acc at step 330: 0.9450\n",
      "test acc at step 340: 0.9482\n",
      "test acc at step 350: 0.9495\n",
      "test acc at step 360: 0.9480\n",
      "test acc at step 370: 0.9426\n",
      "test acc at step 380: 0.9450\n",
      "test acc at step 390: 0.9496\n",
      "Adding run metadata for  399\n",
      "test acc at step 400: 0.9508\n",
      "test acc at step 410: 0.9502\n",
      "test acc at step 420: 0.9499\n",
      "test acc at step 430: 0.9505\n",
      "test acc at step 440: 0.9535\n",
      "test acc at step 450: 0.9538\n",
      "test acc at step 460: 0.9526\n",
      "test acc at step 470: 0.9579\n",
      "test acc at step 480: 0.9547\n",
      "test acc at step 490: 0.9546\n",
      "Adding run metadata for  499\n",
      "test acc at step 500: 0.9582\n",
      "test acc at step 510: 0.9569\n",
      "test acc at step 520: 0.9568\n",
      "test acc at step 530: 0.9557\n",
      "test acc at step 540: 0.9573\n",
      "test acc at step 550: 0.9587\n",
      "test acc at step 560: 0.9584\n",
      "test acc at step 570: 0.9573\n",
      "test acc at step 580: 0.9597\n",
      "test acc at step 590: 0.9597\n",
      "Adding run metadata for  599\n",
      "test acc at step 600: 0.9592\n",
      "test acc at step 610: 0.9575\n",
      "test acc at step 620: 0.9632\n",
      "test acc at step 630: 0.9619\n",
      "test acc at step 640: 0.9618\n",
      "test acc at step 650: 0.9619\n",
      "test acc at step 660: 0.9614\n",
      "test acc at step 670: 0.9608\n",
      "test acc at step 680: 0.9620\n",
      "test acc at step 690: 0.9642\n",
      "Adding run metadata for  699\n",
      "test acc at step 700: 0.9655\n",
      "test acc at step 710: 0.9628\n",
      "test acc at step 720: 0.9621\n",
      "test acc at step 730: 0.9624\n",
      "test acc at step 740: 0.9626\n",
      "test acc at step 750: 0.9638\n",
      "test acc at step 760: 0.9632\n",
      "test acc at step 770: 0.9642\n",
      "test acc at step 780: 0.9658\n",
      "test acc at step 790: 0.9649\n",
      "Adding run metadata for  799\n",
      "test acc at step 800: 0.9670\n",
      "test acc at step 810: 0.9652\n",
      "test acc at step 820: 0.9657\n",
      "test acc at step 830: 0.9672\n",
      "test acc at step 840: 0.9678\n",
      "test acc at step 850: 0.9606\n",
      "test acc at step 860: 0.9609\n",
      "test acc at step 870: 0.9625\n",
      "test acc at step 880: 0.9647\n",
      "test acc at step 890: 0.9656\n",
      "Adding run metadata for  899\n",
      "test acc at step 900: 0.9662\n",
      "test acc at step 910: 0.9682\n",
      "test acc at step 920: 0.9680\n",
      "test acc at step 930: 0.9678\n",
      "test acc at step 940: 0.9676\n",
      "test acc at step 950: 0.9659\n",
      "test acc at step 960: 0.9649\n",
      "test acc at step 970: 0.9664\n",
      "test acc at step 980: 0.9667\n",
      "test acc at step 990: 0.9643\n",
      "Adding run metadata for  999\n"
     ]
    }
   ],
   "source": [
    "# 运行初始化所有变量\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer = tf.summary.FileWriter(log_dir + \"train\", sess.graph)\n",
    "test_writer = tf.summary.FileWriter(log_dir + \"test\")\n",
    "# 开始训练模型\n",
    "for i in range(max_step):\n",
    "    if i % 10 == 0:\n",
    "        summary, acc = sess.run([merged, accuracy], feed_dict=generate_feed(train=False))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print(\"test acc at step %d: %.4f\" % (i, acc))\n",
    "    else:\n",
    "        if i % 100 == 99:  # Record execution stats\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary, _ = sess.run([merged, train_step], \n",
    "                                  feed_dict=generate_feed(train=True), \n",
    "                                  options=run_options, \n",
    "                                  run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, \"step%03d\" % i)\n",
    "            train_writer.add_summary(summary, i)\n",
    "            print(\"Adding run metadata for \", i)\n",
    "        else:  # Record a summary\n",
    "            summary, _ = sess.run([merged, train_step], \n",
    "                                  feed_dict=generate_feed(train=True))\n",
    "train_writer.close()\n",
    "test_writer.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
